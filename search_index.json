[["index.html", "1 Preface", " 1 Preface This is a notebook for reading the book \"Genetics and Analysis of Quantitative Traits (Michael Lynch and Bruce Walsh) "],["an-overview-of-quantitative-genetics.html", "2 An Overview of Quantitative Genetics 2.1 The Adaptationist Approach to Phenotypic Evolution 2.2 Quantitative Genetics and Phenotypic Evolution 2.3 Historical Background 2.4 The Major Goals of Quantitative Genetics 2.5 Mathematics in Biology", " 2 An Overview of Quantitative Genetics 2.1 The Adaptationist Approach to Phenotypic Evolution 2.2 Quantitative Genetics and Phenotypic Evolution The “main principles” has been outlined independently by Ronald Fisher (1918) and Sewall Wright (1921). Theoretical basis for most plant and animal breeding programs for well over a half century. Complex human genetic disorders Evolutionary biology (1980-) Molecular biology Francis Galton: empirical motivation Karl Pearson: formal development of the theory of regression and correlation Ronald Fisher: concept of variance-component partitioning (analsis of varaince, ANOVA), the development of methods for experimental design and hypothesis testing Sewall Wright: path analysis. 2.3 Historical Background 2.4 The Major Goals of Quantitative Genetics 2.4.1 The nature of quantitative-trait variation Partitioning phenotypic variance into its various compoents (Chapters 17-27) the phenotypic resemblance between relatives provides information on the degreee of genetic differentiation among individuals (Chapter 7) Genotype * environment interaction (Chapter 22) There are several components of both environmental and genetic variation (Chapters 4, 5, and 6) The additive component of the genetic variance (the variance of breeding value) is of particluar interest Utimately, the pool of genetic variation in a population must be due to quasi-balance between the forces of selection and random genetic drift, both of which tend to eliminate variation, and the replenishing force of mutation. mutational rate of production of new variation (Chapter 12) the number of loci underlying a character is likely to be large or small (Chapters 9 and 13) molecular-marker-based approaches, QTLs (Chapters 14-16) nonadditive effect (Chapter 5) Chapter 11 2.4.2 The consequences of inbreeding and outcrossing Inbreeding effects are almost always deleterious, generally increasing linearly with the degree os relatedness between parents (Chapter 10) Crosses between isolated lines or populations often exhibit “hybrid vigor” in the F1 generation, only to be followed by substantial fitness decline in the next (F2) generation. (Chapter 9) 2.4.3 The constraints on the evolutionary process Selection opreates on a single trait correlated traits genetic correlation (Chapter 21) 2.4.4 The estimation of breeding values BLUP (Chapter 26) 2.4.5 The development of predictive models for evolutionary change 2.5 Mathematics in Biology "],["properties-of-distributions.html", "3 Properties of Distributions 3.1 Parameters of Univariate Distributions 3.2 The Normal Distribution 3.3 Confidence Intervals", " 3 Properties of Distributions 3.1 Parameters of Univariate Distributions meristic characters (a range of discrete classes) metric characters (continuous scale) all-or-none or binary characters univariate distribution bivariate distribution multivariate distribution pdf: probability density function pmf: probability mass function parameters estimates Statisticians often denote parameters of a population with Greek symbols and to sample estimates with Roman symbols arithmetic mean, first moment about the origin, expected value, expectation population variance, second moment about the mean standard deviation (SD) coefficient of variation (the ratio of the standard deviation to the mean) third moment about the mean: asymmetry of a distribution, skewness coefficient of skewness (a ratio) 3.2 The Normal Distribution Normal distribution/Gaussian distribution (DeMoivre(1738), LaPlace(1778), and Gauss(1809)), the density function is given by \\[p(z) = (2\\pi\\sigma^2)^{-1/2}exp[-\\frac{(z-\\mu)^2}{2\\sigma^2}]\\] central limit theorem Gaussian fitness function (in the theory of stablizing selection) standard normal distribution third moment is 0 fourth moment, kurtosis, leptokurtic vs platykurtic 3.2.1 The truncated normal distribution Under truncation selection, all individuals below a certain phenotype are culled from the population and hence have zero fitness. The critical phenotype, T, is called the truncation point. For a normally distributed pheontype, the density of the phenotype z after selection is \\[\\frac{p(z)}{\\int_T^\\infty p(z)dz}\\] , the mean phenotype of the population above the threshold (i.e., after selection) can be writen as \\[\\mu_s = \\frac{\\int_T^\\infty zp(z)dz}{\\int_T^\\infty p(z)dz} = \\mu+\\frac{\\sigma \\times p_T}{\\Phi_T}\\] , where \\(\\Phi_T\\) is the denominator \\(\\int_T^\\infty p(z)dz\\), a measure of the intensity of selection; \\(p_T\\) is the height of the standard normal curve at the truncation point T. The change in the mean caused by selection (\\(\\mu_s-\\mu = \\frac{\\sigma \\times p_T}{\\Phi_T}\\)) is often denoted by S, the directional selection differential. In a similar fashion, the variance of the selected population can be shown to be \\[\\sigma^2_s = [1+\\frac{p_Tz&#39;}{\\Phi_T}-(\\frac{p_T}{\\Phi_T})^2]\\sigma^2\\] , where \\(z&#39; = T - \\mu\\) mu = 0; sigma=1 phi_T = 10^(seq(-3,0,by=0.05)) Tvalue = qnorm(phi_T, lower.tail=F) p_T = dnorm(Tvalue) mu_s = mu+sigma*p_T/phi_T layout(matrix(1:2, 1, 2)) plot(x=log10(phi_T), y=(mu_s-mu)/sigma, type=&quot;l&quot;, xlab=expression(log[10]~Phi[T]), ylab=expression((mu[s]-mu)/sigma)) plot(x=log10(phi_T), y=(1+p_T*(Tvalue-mu)/phi_T-(p_T/phi_T)^2), type=&quot;l&quot;, xlab=expression(log[10]~Phi[T]), ylab=expression(sigma[s]^2/sigma^2)) 3.3 Confidence Intervals confidence limits or interval standard error (not standard deviation) "],["covariance-regression-and-correlation.html", "4 Covariance, Regression, and Correlation 4.1 Jointly Distributed Random Variables 4.2 Covariance 4.3 Regression 4.4 Correlation 4.5 A Taste of Quantitative-Genetic Theory", " 4 Covariance, Regression, and Correlation In the previous chapter, the variance was introduced as a measure of the dispersion of a univariate distribution. The covariance provides a natural measure of the association between two variables. 4.1 Jointly Distributed Random Variables The probability of joint occurrence of a pair of random variables \\((x,y)\\) is specified by the joint probability density function, \\(p(x,y)\\), where \\[P(y_1 \\le y \\le y_2, x_1 \\le x \\le x_2) = \\int_{y_1}^{y_2}\\int_{x_1}^{x_2}p(x,y)dxdy\\] The conditional density of y given x, \\(p(y|x)\\), \\[P(y_1 \\le y \\le y_2|x) = \\int_{y_1}^{y_2} p(y|x)dy\\] So \\[p(x,y) = p(y|x)p(x)\\] Two random variables, x and y are independent if \\[p(x,y)=p(x)p(y)\\] or \\(p(x|y) = p(x)\\text{ or } p(y|x) = p(y)\\) 4.1.1 Expectations of jointly distributed variables In general, conditional expectations are computed by using the conditional desnity \\[E(y|x)=\\int_{-\\infty}^{+\\infty}yp(y|x)dy\\] 4.2 Covariance The covariance of x and y is defined as \\[\\sigma(x,y) = E[(x-\\mu_x)(y-\\mu_y)] = E(xy)-\\mu_x\\mu_y\\] , where \\(\\mu_x\\) and \\(\\mu_y\\) are the mean of variable x and y, respectively. We often denote covariance by \\(\\sigma_{x,y}\\). The sampling estimator of \\(\\sigma_{x,y}\\) is \\[Cov(x,y)=\\frac{n(\\bar{xy}-\\bar{x} \\times \\bar{y})}{n-1}\\] , where n is the number of paris of observations, \\(\\bar{xy} = \\frac{1}{n}\\sum_{i=1}^nx_iy_i\\), \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\) and \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i\\) If x and y are independent, then \\(\\sigma(x,y)=0\\), but the converse is not ture. 4.2.1 Useful identities for variances and covariances \\[\\sigma(x,y) = \\sigma(y,x)\\] \\[\\sigma(x,x)=\\sigma^2(x)\\] For any constant a, \\[\\sigma(a, x) = 0\\] \\[\\sigma(ax, y) = a\\sigma(x,y)\\] For another constant b, \\[\\sigma(ax, by) = ab\\sigma(x,y)\\] \\[\\sigma^2(ax) = a^2\\sigma^2(x)\\] \\[\\sigma[(a+x),y] = \\sigma(x,y)\\] For four variables x, y, w and z, \\[\\sigma[(x+y),(w+z)] = \\sigma(x,w)+\\sigma(y,w)+\\sigma(x,z)+\\sigma(y,z)\\] \\[\\sigma^2(x+y) = \\sigma^2(x) + \\sigma^2(y) + 2\\sigma(x,y)\\] 4.3 Regression \\[y = \\alpha+\\beta x + e\\] x, predictor/independent variable y, response/dependent variable \\(\\alpha\\), intercept \\(\\beta\\), regression coefficient \\(e\\), residual error Usually, \\(\\hat{y}\\), a, and b denote the estimator of y, \\(\\alpha\\) and \\(\\beta\\) 4.3.1 Derivation of the least-squares linear regression Least-squares linear regression other criteria: absolute deviations, cubed deviations The least-squares estomators for the intercept and slope of a linear regression: \\[a = \\bar{y} - b\\bar{x}\\] \\[b = \\frac{Cov(x,y)}{Var(x)}\\] ### Properties of least-squares regression The regreesion line passes through the means of both x and y. The average value of the residual is zero. For any set of paired data, the least-squares regression parameters, a and b define the straight line that maximizes the amount of variantion in y that can be explained by a linear regression on x. The residual errors around the least-squares regression are uncorrelated with the predictor variable x. The true regreesion, the value of \\(E(y|x)\\), is both linear and homoscedastic - when x and y are bivariate normally distributed. The regression of y on x is different from the regression on y unless the means and variances of the two variables are equal. 4.4 Correlation The correlation coefficient is defined by \\[r(x,y) = \\frac{Cov(x,y)}{\\sqrt{Var(x)Var(y)}}\\] The parametric correlation coefficient is denoted by \\(\\rho(x,y)\\) (or \\(\\rho\\)) and equalts \\(\\sigma(x,y)/\\sigma(x)\\sigma(y)\\). The least-squares regression coefficient is related to the correlation coefficient by \\[b(y,x) = r\\sqrt{\\frac{Var(y)}{Var(x)}}\\] An advantage of correlations over covariances is that the former are scale independent. \\[r(wx, cy) = r(x,y)\\] Since r is dimensionless with limits of \\(\\pm 1\\), it gives a direct measure of the degress of association: if |r| is close t one, x and y are very stronly associated in a linear fashion, while if |r| is close to zero, they are not. Other propeties: r is a standardized regression coefficient (the regression coefficient resulting from rescaling x and y such that each has unit variance). The squared correlation coefficient measures the proportion of the variance in y that is explained by assuming that E(y|x) is linear. The variance of the response variable y has two components: \\(r^2Var(y)\\), the amount of variance accounted for by the linear model (the regression variance), and \\((1-r^2)Var(y)\\), the remaining variance not accountable by the regression (the residual variance). 4.5 A Taste of Quantitative-Genetic Theory 4.5.1 Directional selection differentials and the Robertson-Price identity The evolutionary response of a character to selection is a function of the intensity of selection and the fraction of the phenotypic variance attributable to certain genetic effects. mean individual fitness Robertson-Price identity breeders’ equation 4.5.2 The correlation between genotypic and phenotypic values 4.5.3 Regression of offspring pheotype on parental phenotype "],["properties-of-single-loci.html", "5 Properties of Single Loci 5.1 Allele and Genotype Frequencies 5.2 The Transmission of Genetic Information 5.3 Characterizzing the influence of a Locus on the Phenotype 5.4 The Basis of Dominance 5.5 Fisher’s Decomposition of the Genetypic Value 5.6 Partitioning the Genetic Variance 5.7 Additive Effects, Average Excesses, and Breeding Values 5.8 Extensions to Multiple Alleles and Nonrandom Mating", " 5 Properties of Single Loci 5.1 Allele and Genotype Frequencies 5.2 The Transmission of Genetic Information 5.2.1 The Hardy-Weinberg principle 5.2.2 Sex-linked loci 5.2.3 Polyploidy 5.2.4 Age structure 5.2.5 Testing for Hardy-Weinberg propertions 5.3 Characterizzing the influence of a Locus on the Phenotype 5.4 The Basis of Dominance 5.5 Fisher’s Decomposition of the Genetypic Value 5.6 Partitioning the Genetic Variance 5.7 Additive Effects, Average Excesses, and Breeding Values 5.8 Extensions to Multiple Alleles and Nonrandom Mating 5.8.1 Average excess 5.8.2 Additive effects 5.8.3 Additive genetic variance "],["sources-of-genetic-variation-for-multilocus-traits.html", "6 Sources of Genetic Variation for Multilocus Traits 6.1 Epistasis 6.2 A General Least-Squares Model for Genetic Effects 6.3 Linkage 6.4 Effect of Diseequilibrium on the Genetic Variance", " 6 Sources of Genetic Variation for Multilocus Traits 6.1 Epistasis 6.2 A General Least-Squares Model for Genetic Effects 6.2.1 Extension to haploids and polyploids 6.3 Linkage 6.3.1 Estimation of gametic phase disequilibrium 6.4 Effect of Diseequilibrium on the Genetic Variance 6.4.1 The evidence "],["components-of-environmental-variation.html", "7 Components of Environmental Variation 7.1 Extension of the Linear Models to Phenotypes 7.2 Special Environmental Effects 7.3 General Environmental Effects of Maternal Influence 7.4 Genotype * Environment Interaction", " 7 Components of Environmental Variation 7.1 Extension of the Linear Models to Phenotypes 7.2 Special Environmental Effects 7.2.1 Within-individual variations 7.2.2 Developmental homeostasis and homozygosity 7.2.3 Repeatability 7.3 General Environmental Effects of Maternal Influence 7.4 Genotype * Environment Interaction "],["resemblance-between-realtives.html", "8 Resemblance between Realtives 8.1 Measures of Relatedness 8.2 The Genetic Covariance between Relatives 8.3 The effects of Linkage and Gametic Phase Disequilibrium 8.4 Assortative Mating 8.5 Polyploidy 8.6 Environmental Sources of Covariance Between Relatives 8.7 The Heritability Concept", " 8 Resemblance between Realtives 8.1 Measures of Relatedness 8.1.1 Coefficients of identity 8.1.2 Coefficients of coancestry and inbreeding 8.1.3 The coefficient of fraternity 8.2 The Genetic Covariance between Relatives 8.3 The effects of Linkage and Gametic Phase Disequilibrium 8.3.1 Linkage 8.3.2 Gametic phase disequilibrium 8.4 Assortative Mating 8.5 Polyploidy 8.6 Environmental Sources of Covariance Between Relatives 8.7 The Heritability Concept 8.7.1 Evolvability "],["introduction-of-matrix-algebra-and-linear-models.html", "9 Introduction of Matrix Algebra and Linear Models 9.1 Multiple Regression 9.2 Elementary Matrix Algebra 9.3 Expectations of Random Vectors and Matrices 9.4 Covariance Matrices of Transformed Vectors 9.5 The Multivariate Normal Distribution 9.6 Overview of Linear Models", " 9 Introduction of Matrix Algebra and Linear Models 9.1 Multiple Regression 9.1.1 An application to multivariate selection 9.2 Elementary Matrix Algebra 9.2.1 Basic notation 9.2.2 Partitioned matrices 9.2.3 Addition and subtraction 9.2.4 Multiplication 9.2.5 Transposition 9.2.6 Inverses and solutions to systems of equations 9.2.7 Determinants and minors 9.2.8 Computing inverses 9.3 Expectations of Random Vectors and Matrices 9.4 Covariance Matrices of Transformed Vectors 9.5 The Multivariate Normal Distribution 9.5.1 Properties of the MVN 9.6 Overview of Linear Models 9.6.1 Ordinary Least squares 9.6.2 Generalized least squares "],["estimation-of-breeding-values.html", "10 Estimation of Breeding Values 10.1 The General Mixed Model 10.2 Models for the Estimation of Breeding Values 10.3 Sample Rules for Computing \\(A\\) and \\(A^{-1}\\) 10.4 Joint Estimation of Several Vectors of Random Effects", " 10 Estimation of Breeding Values All of the methods to be covered are based on the general mixed model. This chapter introduces best linear unbiased prediction (BLUP), a general method for predicting random effects (such as breeding values and maternal effects), while Chapter 27 is concerned with the estiamtion of genetic variances by restricted maximum likelihoold (REML). These two methods are related in that BLUP assumes that the appropriate variance components are known, while REML procedures estimate variance components in an iterative fashion from BLUP estimates of random effects. 10.1 The General Mixed Model \\[\\bf{y = X}\\pmb{\\beta}+\\bf{Zu+e}\\] \\(\\bf{y}\\), a column vector containing the phenotypic values for a trait measured in n individuals \\(\\pmb{\\beta}\\), a \\(p \\times 1\\) vector of fixed effects \\(\\bf{u}\\), a \\(q \\times 1\\) vector of random effects \\(\\bf{X}\\), \\(n \\times p\\) incidence matrices (also called the design matrix) \\(\\bf{Z}\\), \\(n \\times q\\) incidence matrices \\(\\bf{e}\\), a \\(n times 1\\) column vector of residual deviations assume to be distributed independently of the random genetic effects Because the model jointly accounts for fixed and random effects, it is generally referred to as a mixed model (Eisenhart 1947). Example 1 Let \\(y_{ijk}\\) denote the phenotypic value of the \\(k\\)th offspring of sire \\(i\\) in environment \\(j\\). Obersvation Value Sire Environment \\(y_{111}\\) 9 1 1 \\(y_{121}\\) 12 1 2 \\(y_{211}\\) 11 2 1 \\(y_{212}\\) 6 2 1 \\(y_{311}\\) 7 3 1 \\(y_{321}\\) 14 3 2 So the resulting vector/matrix in the mixed model are \\(\\pmb{y} = \\begin{pmatrix} y_{111} \\\\ y_{121} \\\\ y_{211} \\\\ y_{212} \\\\ y_{311} \\\\ y_{321} \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 12 \\\\ 11 \\\\ 6 \\\\ 7 \\\\14 \\end{pmatrix}\\), \\(\\pmb{X} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; 0\\\\ 1 &amp; 0\\\\ 1 &amp; 0\\\\ 0 &amp; 1 \\end{pmatrix}\\), \\(\\pmb{Z} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{pmatrix}\\), \\(\\pmb{\\beta}=\\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}\\), \\(\\pmb{u} = \\begin{pmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{pmatrix}\\) Now assume \\(\\pmb{u} \\sim (0, \\pmb{G})\\) and \\(\\pmb{e} \\sim (0, \\pmb{R})\\), then the covariance matrix for the vector of observations \\(\\bf{y}\\) is \\[Var(\\pmb{y}) = \\bf{V} = ZGZ^T+R\\] Generally we assume the residual errors have constant variance and are uncorrelated, so \\(\\pmb{R} = \\sigma_E^2\\pmb{I}\\) (a diagonal matrix). For the mixed model, we observe \\(\\bf{y}\\), \\(\\bf{X}\\), and \\(\\bf{Z}\\), while \\(\\pmb{\\beta}\\), \\(\\pmb{u}\\), \\(\\pmb{R}\\), and \\(\\pmb{G}\\) are genrally unknown. Thus, mixed-model analysis involves two complementary estimation issues: estimation of he vectors of fixed and random effects \\(\\pmb{\\beta}\\) and \\(\\pmb{u}\\) (chapter 26) estimation of the covariance matrices \\(\\pmb{R}\\) and \\(\\pmb{G}\\) (chapter 27) 10.1.1 Estimating Fixed Effects and Predicting Random Effects Inferences about fixed effects have come to be called estimates, whereas those that concern random effects are known as predictions. The most widely used procedures are BLUE (best linear unbiased estimator) and BLUP (best linear unbiased predictor). The “best” means they minimize the sampling variance, “linear” means they are linear functions of the obersved phenotypes \\(\\bf{y}\\), and “unbiased” means \\(E[BLUE(\\pmb{\\beta})] = \\pmb{\\beta}\\) and \\(E[BLUP(\\pmb{u})] = \\pmb{u}\\). BLUE \\[\\pmb{\\hat{\\beta}} = \\bf{(X^TV^{-1}X)^{-1}X^TV^{-1}y}\\] BLUP \\[\\bf{\\hat{u} = GZ^TV^{-1}(y-X\\pmb{\\beta})}\\] The practical application of both of these expressions requires that the variance components be known. Thus, prior to a BLUP analysis, the variance components need to be estimated by ANOVA or REML. As BLUE and BLUP require the inverse of the covariance matrix \\(\\bf{V^{-1}}\\), when \\(\\bf{y}\\) contains many thousands of observations, the computation of \\(\\bf{V^{-1}}\\) can be quite difficult. Henderson offered a more compact method for jointly obtaining \\(\\pmb{\\hat{\\beta}}\\) and \\(\\bf{\\hat{u}}\\) in the form of his mixed-model equations (MME), \\[\\begin{pmatrix}\\bf{ X^TR^{-1}X} &amp; \\bf{ X^TR^{-1}Z} \\\\ \\bf{ Z^TR^{-1}X} &amp; \\bf{ Z^TR^{-1}Z+G^{-1}} \\end{pmatrix} \\begin{pmatrix} \\pmb{\\hat{\\beta}} \\\\ \\bf{\\hat{u}} \\end{pmatrix} = \\begin{pmatrix} \\bf{X^TR^{-1}y} \\\\ \\bf{Z^TR^{-1}y} \\end{pmatrix}\\] As \\(\\bf{R}\\) and \\(\\bf{G}\\) are usually diagonal, \\(\\bf{R^{-1}}\\) and \\(\\bf{G^{-1}}\\) are trivial to obtain. In addition, the leftmost matrix has a dimension \\((p+q) \\times (p+q)\\), which is usually considerably less than the dimensionality of \\(\\bf{V}\\) (an \\(n \\times n\\) matrix). 10.1.2 Estimability of fixed factors generalized inverse 10.1.3 Standard Errors A relatively straightforward extension of Henderson’s mixed-model equations provides estimates of the standard errors of the fixed and random effects. Let the inverse of the leftmost matrix in MME be \\[\\begin{pmatrix}\\bf{ X^TR^{-1}X} &amp; \\bf{ X^TR^{-1}Z} \\\\ \\bf{ Z^TR^{-1}X} &amp; \\bf{ Z^TR^{-1}Z+G^{-1}} \\end{pmatrix}^{-1} = \\begin{pmatrix} \\bf{C_{11}} &amp; \\bf{C_{12}} \\\\ \\bf{C_{12}^T} &amp; \\bf{C_{22}} \\end{pmatrix}\\] , where \\(\\bf{C_{11}}\\), \\(\\bf{C_{12}}\\), and \\(\\bf{C_{22}}\\) are, respectively, \\(p \\times p\\), \\(p \\times q\\), and \\(q \\times q\\) submatrices. Henderson (1975) showed that the sampling covariance matrix for the BLUE of \\(\\beta\\) is given by \\[\\sigma(\\pmb{\\hat{\\beta}}) = \\bf{C_{11}}\\] , that the sampling covariance matrix of the prediction errors \\((\\bf{\\hat{u} - u})\\) is given by \\[\\sigma(\\bf{\\hat{u} - u}) = C_{22}\\] , and that the sampling covariance of the estimated effects and prediction errors is given by \\[\\sigma(\\pmb{\\hat{\\beta}},\\bf{\\hat{u} - u}) = C_{12}\\] 10.2 Models for the Estimation of Breeding Values Three specific varaints of the general mixed model: Animal models estimate the breeding values of each measured individual; Gametic models describe the breeding values of measured individuals in teerms of parental contributions; Reduced animal model combines aspects of both the animal and gametic models in specific applications in which parental breeding values are the only ones of interest. 10.2.1 The animal model Assuming only a single fixed factor (the population mean) under the simplest animal model, the observation for individual i is expressed as \\[y_i=\\mu+a_i+e_i\\] , where \\(a_i\\) is the additive genetic value of individual i. The matrix \\(\\pmb{G}=\\sigma^2_A\\pmb{A}\\) is the additive genetic (or numerator) relationship matrix. Assuming \\(\\pmb{R}=\\sigma_E^2\\pmb{I}\\), the mixed-model equation reduces to \\[\\begin{pmatrix}\\pmb{X}^T\\pmb{X} &amp; \\pmb{X}^T\\pmb{Z} \\\\ \\pmb{Z}^T\\pmb{X} &amp; \\pmb{Z}^T\\pmb{Z}+\\lambda\\pmb{A}^{-1}\\end{pmatrix}\\begin{pmatrix}\\hat{\\beta} \\\\ \\hat{\\pmb{u}}\\end{pmatrix}=\\begin{pmatrix} \\pmb{X}^T\\pmb{y} \\\\ \\pmb{Z}^T\\pmb{y}\\end{pmatrix}\\] where \\(\\lambda=\\sigma^2_E/\\sigma^2_A=(1-h^2)/h^2\\) Further assume =$ (each individuals has only a single observation), the mixed-model equation reduces to \\[\\begin{pmatrix}n &amp; \\pmb{1}^T \\\\ \\pmb{1} &amp; \\pmb{I}+\\lambda\\pmb{A}^{-1}\\end{pmatrix}\\begin{pmatrix}\\hat{\\mu} \\\\ \\hat{\\pmb{u}}\\end{pmatrix}=\\begin{pmatrix}\\sum^ny_i \\\\ \\pmb{y}\\end{pmatrix}\\] 10.2.2 The gametic model 10.2.3 The reduced animal model 10.3 Sample Rules for Computing \\(A\\) and \\(A^{-1}\\) 10.3.1 Allowing for mutation when computing A 10.4 Joint Estimation of Several Vectors of Random Effects 10.4.1 BLUP estimates of dominance values 10.4.2 Repeated records 10.4.3 Maternal effects 10.4.4 Multiple traits "],["variance-component-estimation-with-complex-pedigrees.html", "11 Variance-component estimation with complex pedigrees", " 11 Variance-component estimation with complex pedigrees "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
